{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data avec Spark : Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Nom & Prenom : ABABACAR SAGNA`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Datasets** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données [ici](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**\n",
    "\n",
    "**Download csv file [here](https://data.sfgov.org/api/views/nuek-vuh3/rows.csv?accessType=DOWNLOAD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce travail est de comprendre le Dataset SF Fire afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquats.\n",
    "\n",
    "- Code lisible et bien indenté, \n",
    "- N'oublier pas de mettre en commentaire la justification de votre réponse sur les cellule Markdown. \n",
    "\n",
    "\n",
    "#### Note:\n",
    "- Vous pouvez en groupe (au plus deux étudiants) . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Importez les modules Spark necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.DataFrame\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9`\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mrootLogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.spi.RootLogger@5065bd94"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val rootLogger = Logger.getRootLogger()\n",
    "rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.spark-project\").setLevel(Level.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Creez la Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 16:55:20 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@573f5e2f"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder\n",
    "  .master(\"local\")\n",
    "  .appName(\"Mon-Projet-spark\")\n",
    "  .config(\"spark.some.config.option\", \"config-value\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Chargez les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le `fireSchema` definit dans la cellule suivante pour le chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mfireSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"IncidentNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"WatchDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallFinalDisposition\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"AvailableDtTm\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"City\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Zipcode\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Battalion\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StationArea\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Box\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"OriginalPriority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Priority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FinalPriority\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ALSUnit\"\u001b[39m, BooleanType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallTypeGroup\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"NumAlarms\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitSequenceInCallDispatch\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FirePreventionDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"SupervisorDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Neighborhood\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Location\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"RowID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Delay\"\u001b[39m, FloatType, true, {})\n",
       ")\r\n",
       "\u001b[36msfFireData\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))\n",
    "\n",
    "// your code here (hint spark session name is sparkSession Q2)\n",
    "val sfFireData = spark.read.option(\"header\", \"false\").schema(fireSchema).csv(\"sf-fire-calls.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Mettez en cache les donnees chargees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// dataframe_name.cache\n",
    "val data = sfFireData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la mise en cache quand on effectue plusieurs actions sur le même DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Supprimez tous les appels de type `Medical Incident`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: appliquez la methode `.filter()` a la colonne `CallType` avec l'operateur `=!=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\n",
       "\n",
       "/* Utilisation de la methode .filter pour filtrer par rapport a la colonne \"CallType\" et retirer toutes les lignes qui ont le\n",
       "    d'appel  \"Medical Incident\"\n",
       "*/\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mfilterSFDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: int, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Importer le module necessaire\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "/* Utilisation de la methode .filter pour filtrer par rapport a la colonne \"CallType\" et retirer toutes les lignes qui ont le\n",
    "    d'appel  \"Medical Incident\"\n",
    "*/\n",
    "\n",
    "val filterSFDF = data.filter(col(\"CallType\") =!= \"Medical Incident\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Combien de types d'appels distincts ont été passés ?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT CallType)|\n",
      "+------------------------+\n",
      "|                      30|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.countDistinct\n",
       "/* Cette methode ~agg(countDistinct)~ permet de recuperer les types distincts et d'aggreger de \"1\" (+1), \n",
       "lorsque le type rencontree est unique\n",
       "*/\n",
       "\u001b[39m\r\n",
       "\u001b[36mdistinctsCallType\u001b[39m: \u001b[32mDataFrame\u001b[39m = [count(DISTINCT CallType): bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "/* Cette methode ~agg(countDistinct)~ permet de recuperer les types distincts et d'aggreger de \"1\" (+1), \n",
    "lorsque le type rencontree est unique\n",
    "*/\n",
    "val distinctsCallType = data.agg(countDistinct(\"CallType\"))\n",
    "distinctsCallType.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Quels types d'appels  ont été passés au service d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|null                                        |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Cette methode permet de les afficher (Les types distincts de la colonne \"CallType\")\n",
    "\n",
    "/* Ici on precise 30 dans la methode .show() pour afficher les 20 \"CallType\" */\n",
    "\n",
    "data.select(\"CallType\").distinct.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Trouvez toutes les réponses ou les délais sont supérieurs à 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "1. Renommez la colonne `Delay` -> `ReponseDelayedinMins`\n",
    "2. Retournez un nouveau DataFrame\n",
    "3. Affichez tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|ReponseDelayedinMins|Reponse_gt_5|\n",
      "+--------------------+------------+\n",
      "|                5.35|        true|\n",
      "|                6.25|        true|\n",
      "|                 5.2|        true|\n",
      "|                 5.6|        true|\n",
      "|                7.25|        true|\n",
      "|           11.916667|        true|\n",
      "|            5.116667|        true|\n",
      "|            8.633333|        true|\n",
      "|            95.28333|        true|\n",
      "|                5.45|        true|\n",
      "|                 7.6|        true|\n",
      "|            6.133333|        true|\n",
      "|           5.1833334|        true|\n",
      "|           6.9166665|        true|\n",
      "|                 5.2|        true|\n",
      "|                6.35|        true|\n",
      "|            7.983333|        true|\n",
      "|               13.55|        true|\n",
      "|                5.15|        true|\n",
      "|           13.583333|        true|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 26 more fields]\r\n",
       "\u001b[36mdfa\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 27 more fields]\r\n",
       "\u001b[36mdelays_gt_5\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: int, UnitID: string ... 27 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Renommons la colonne ~Delay~ en ~ReponseDelaydinmins~, puis retournons un nouveau df\n",
    "val df = data.withColumnRenamed(\"Delay\",\"ReponseDelayedinMins\")\n",
    "\n",
    "// Selectionner tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes\n",
    "val dfa = df.withColumn(\"Reponse_gt_5\", col(\"ReponseDelayedinMins\").gt(5))\n",
    "\n",
    "// Afficher le résultat\n",
    "\n",
    "val delays_gt_5 = dfa.filter(col(\"ReponseDelayedinMins\").gt(5))\n",
    "delays_gt_5.select(\n",
    "    \"ReponseDelayedinMins\",\n",
    "    \"Reponse_gt_5\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Convertissez les colonnes dates en timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "* `CallDate` -> `IncidentDate`\n",
    "* `WatchDate` -> `OnWatchDate`\n",
    "* `AvailableDtTm` -> `AvailableDtTS`\n",
    "exemple code pour le cas de `CallDate`:\n",
    "`dataframe.withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|null               |null               |null               |\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 00:00:00|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.to_timestamp\n",
       "\n",
       "// CallDate\n",
       "\u001b[39m\r\n",
       "\u001b[36mdfb\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 27 more fields]\r\n",
       "\u001b[36mdfc\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 27 more fields]\r\n",
       "\u001b[36mdfd\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 27 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Importons le module necessaire pour cette transformation\n",
    "import org.apache.spark.sql.functions.to_timestamp\n",
    "\n",
    "// CallDate\n",
    "val dfb = dfa.withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")\n",
    "\n",
    "// WatchDate\n",
    "val dfc = dfb.withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    "\n",
    "// AvailableDtTm\n",
    "val dfd = dfc.withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy\")).drop(\"AvailableDtTm\")\n",
    "\n",
    "// Display the result\n",
    "dfd.select(\n",
    "    \"IncidentDate\",\n",
    "    \"OnWatchDate\",\n",
    "    \"AvailableDtTS\"\n",
    ").show(2,false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Quels sont les types d'appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+-----------+\n",
      "|CallType                       |count |num_gt_5000|\n",
      "+-------------------------------+------+-----------+\n",
      "|Medical Incident               |113794|true       |\n",
      "|Structure Fire                 |23319 |true       |\n",
      "|Alarms                         |19406 |true       |\n",
      "|Traffic Collision              |7013  |true       |\n",
      "|Citizen Assist / Service Call  |2524  |false      |\n",
      "|Other                          |2166  |false      |\n",
      "|Outside Fire                   |2094  |false      |\n",
      "|Vehicle Fire                   |854   |false      |\n",
      "|Gas Leak (Natural and LP Gases)|764   |false      |\n",
      "|Water Rescue                   |755   |false      |\n",
      "|Odor (Strange / Unknown)       |490   |false      |\n",
      "|Electrical Hazard              |482   |false      |\n",
      "|Elevator / Escalator Rescue    |453   |false      |\n",
      "|Smoke Investigation (Outside)  |391   |false      |\n",
      "|Fuel Spill                     |193   |false      |\n",
      "|HazMat                         |124   |false      |\n",
      "|Industrial Accidents           |94    |false      |\n",
      "|Explosion                      |89    |false      |\n",
      "|Train / Rail Incident          |57    |false      |\n",
      "|Aircraft Emergency             |36    |false      |\n",
      "+-------------------------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcountDistinctCallType\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallType: string, count: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// On recupere le nombre de type d'appel, pour chaque type d'appel\n",
    "val countDistinctCallType =  dfd.groupBy(\"CallType\").count()\n",
    "// countDistinctCallType.orderBy(\"count\").show(35,false)\n",
    "\n",
    "\n",
    "// show(35,false) : pour afficher toutes les valeurs, sans restriction\n",
    "\n",
    "countDistinctCallType\n",
    "  .withColumn(\"num_gt_5000\", col(\"count\").gt(5000))\n",
    "  .show(5,false)\n",
    "\n",
    "val countDistinctCallType =  dfd.groupBy(\"CallType\").count()\n",
    "\n",
    "countDistinctCallType\n",
    "  .withColumn(\"num_gt_5000\", col(\"count\").gt(5000))\n",
    "  .orderBy(col(\"count\").desc)\n",
    "  .show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. Quels sont les boites postales rencontrées dans les appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Zipcode|count|\n",
      "+-------+-----+\n",
      "|  94102|21840|\n",
      "|  94103|20897|\n",
      "|  94110|14801|\n",
      "|  94109|14686|\n",
      "|  94124| 9236|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+\n",
      "|Zipcode|\n",
      "+-------+\n",
      "|94109  |\n",
      "|94115  |\n",
      "|94112  |\n",
      "|94127  |\n",
      "|94108  |\n",
      "|94121  |\n",
      "|94105  |\n",
      "|null   |\n",
      "|94131  |\n",
      "|94116  |\n",
      "|94134  |\n",
      "|94124  |\n",
      "|94102  |\n",
      "|94114  |\n",
      "|94107  |\n",
      "|94111  |\n",
      "|94103  |\n",
      "|94117  |\n",
      "|94122  |\n",
      "|94110  |\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfZipcode\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Zipcode: int, count: bigint]\r\n",
       "\u001b[36mfrequentCallTypes\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"Medical Incident\"\u001b[39m,\n",
       "  \u001b[32m\"Structure Fire\"\u001b[39m,\n",
       "  \u001b[32m\"Alarms\"\u001b[39m,\n",
       "  \u001b[32m\"Traffic Collision\"\u001b[39m\n",
       ")\r\n",
       "\u001b[36mfilteredData\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: int, UnitID: string ... 27 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculer le nombre d'appels par code postal\n",
    "val dfZipcode = dfd.groupBy(\"Zipcode\").count()\n",
    "\n",
    "// Afficher les 5 codes postaux les plus fréquents\n",
    "dfZipcode.orderBy($\"count\".desc).show(5)\n",
    "\n",
    "// Filtrer les appels par type d'appel le plus fréquent\n",
    "val frequentCallTypes = Seq(\"Medical Incident\", \"Structure Fire\", \"Alarms\", \"Traffic Collision\")\n",
    "val filteredData = dfd.filter($\"CallType\".isin(frequentCallTypes:_*))\n",
    "\n",
    "// Afficher les codes postaux distincts des appels filtrés\n",
    "filteredData.select(\"Zipcode\").distinct().show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. Quels sont les quartiers de San Francisco dont les codes postaux sont `94102` et `94103`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Address|\n",
      "+--------------------+\n",
      "|MARKET ST/MCALLIS...|\n",
      "|600 Block of POLK ST|\n",
      "|    9TH ST/HOWARD ST|\n",
      "|400 Block of VALE...|\n",
      "|  16TH ST/MISSION ST|\n",
      "|   4TH ST/MISSION ST|\n",
      "|400 Block of TURK ST|\n",
      "|   OAK ST/WEBSTER ST|\n",
      "| 0 Block of JONES ST|\n",
      "|400 Block of EDDY ST|\n",
      "|300 Block of CLEM...|\n",
      "| 500 Block of OAK ST|\n",
      "|700 Block of MARK...|\n",
      "|HAIGHT ST/OCTAVIA ST|\n",
      "|100 Block of JULI...|\n",
      "|0 Block of LARKIN ST|\n",
      "|100 Block of TURK ST|\n",
      "|CALL BOX: BUCHANA...|\n",
      "|    5TH ST/MARKET ST|\n",
      "| 100 Block of 7TH ST|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// On fait un filtre avec ces deux \"Zipcode\" specifiquements \n",
    "dfd.filter($\"Zipcode\" === 94102 || $\"Zipcode\" === 94103).select(\"Address\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. Determinez le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+------+\n",
      "|CallType                                    |count |\n",
      "+--------------------------------------------+------+\n",
      "|Elevator / Escalator Rescue                 |453   |\n",
      "|Marine Fire                                 |14    |\n",
      "|Aircraft Emergency                          |36    |\n",
      "|Confined Space / Structure Collapse         |13    |\n",
      "|Administrative                              |3     |\n",
      "|Alarms                                      |19406 |\n",
      "|Odor (Strange / Unknown)                    |490   |\n",
      "|null                                        |1     |\n",
      "|Citizen Assist / Service Call               |2524  |\n",
      "|HazMat                                      |124   |\n",
      "|Watercraft in Distress                      |28    |\n",
      "|Explosion                                   |89    |\n",
      "|Oil Spill                                   |21    |\n",
      "|Vehicle Fire                                |854   |\n",
      "|Suspicious Package                          |15    |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|28    |\n",
      "|Other                                       |2166  |\n",
      "|Outside Fire                                |2094  |\n",
      "|Traffic Collision                           |7013  |\n",
      "|Assist Police                               |35    |\n",
      "|Gas Leak (Natural and LP Gases)             |764   |\n",
      "|Water Rescue                                |755   |\n",
      "|Electrical Hazard                           |482   |\n",
      "|High Angle Rescue                           |32    |\n",
      "|Structure Fire                              |23319 |\n",
      "|Industrial Accidents                        |94    |\n",
      "|Medical Incident                            |113794|\n",
      "|Mutual Aid / Assist Outside Agency          |9     |\n",
      "|Fuel Spill                                  |193   |\n",
      "|Smoke Investigation (Outside)               |391   |\n",
      "|Train / Rail Incident                       |57    |\n",
      "+--------------------------------------------+------+\n",
      "\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|avg(ReponseDelayedinMins)|min(ReponseDelayedinMins)|max(ReponseDelayedinMins)|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "|        3.892364154521585|              0.016666668|                  1844.55|\n",
      "+-------------------------+-------------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mresponseStats\u001b[39m: \u001b[32mDataFrame\u001b[39m = [avg(ReponseDelayedinMins): double, min(ReponseDelayedinMins): float ... 1 more field]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// le nombre total d'appels\n",
    "dfd.groupBy(\"CallType\").count().show(31,false)\n",
    "\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val responseStats = dfd.agg(avg(\"ReponseDelayedinMins\"), min(\"ReponseDelayedinMins\"), max(\"ReponseDelayedinMins\"))\n",
    "responseStats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. Combien d'années distinctes trouve t-on dans ce Dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Appliquer la fonction `year()` a la colonne `IncidentDate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT year)|\n",
      "+--------------------+\n",
      "|                  19|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfe\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 28 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// On cree une nouvelle colonne \"year\"\n",
    "val dfe = dfd.withColumn(\"year\", year(to_timestamp($\"IncidentDate\", \"MM/dd/yyyy\")))\n",
    "\n",
    "// On recupere tous les annees distincts par aggregation\n",
    "dfe.agg(countDistinct(\"year\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|week_of_year|count|\n",
      "+------------+-----+\n",
      "|22          |272  |\n",
      "+------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdff\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 29 more fields]\r\n",
       "\u001b[36mdfIncidentByWeek\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [week_of_year: string, count: bigint]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "// Créer une colonne avec le format semaine \"week_of_year\"\n",
    "val dff = dfe.withColumn(\"week_of_year\", date_format(col(\"IncidentDate\"), \"w\"))\n",
    "\n",
    "// Calculer le nombre d'incidents par semaine en 2018\n",
    "val dfIncidentByWeek = dff.filter(year($\"IncidentDate\") === 2018)\n",
    "  .groupBy($\"week_of_year\")\n",
    "  .agg(count($\"IncidentDate\").as(\"count\"))\n",
    "  .orderBy(desc(\"count\"))\n",
    "\n",
    "// Afficher la semaine avec le plus grand nombre d'appels d'incendie\n",
    "dfIncidentByWeek.show(1, false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|Neighborhood|AvgResponseTime|\n",
      "+------------+---------------+\n",
      "|McLaren Park|0.0            |\n",
      "|Seacliff    |0.0            |\n",
      "|Excelsior   |0.0            |\n",
      "|Lincoln Park|0.0            |\n",
      "|None        |0.0            |\n",
      "+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf2018\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: int, UnitID: string ... 28 more fields]\r\n",
       "\u001b[36mdfResponseTime\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: int, UnitID: string ... 29 more fields]\r\n",
       "\u001b[36mdfResponseByNeighborhood\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Neighborhood: string, AvgResponseTime: double]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Filtrer les données pour ne garder que celles de l'année 2018\n",
    "val df2018 = dfe.filter(year(col(\"IncidentDate\")) === 2018)\n",
    "\n",
    "// Calculer le temps de réponse pour chaque appel\n",
    "val dfResponseTime = df2018.withColumn(\"ResponseTime\", unix_timestamp(col(\"AvailableDtTS\")) - unix_timestamp(col(\"IncidentDate\")))\n",
    "\n",
    "// Agréger les temps de réponse par quartier et calculer la moyenne\n",
    "val dfResponseByNeighborhood = dfResponseTime.groupBy(col(\"Neighborhood\")).agg(avg(col(\"ResponseTime\")).as(\"AvgResponseTime\"))\n",
    "\n",
    "// Trier les quartiers par temps de réponse moyen croissant et afficher les 5 premiers\n",
    "dfResponseByNeighborhood.orderBy(col(\"AvgResponseTime\").asc).show(5, false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Stocker les données sous format de fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/12 19:11:31 ERROR Executor: Exception in task 0.0 in stage 124.0 (TID 4122)\n",
      "java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmmpp\\fiiredataService_parquet\\fiiles\\_temporary\\0\\_temporary\\attempt_20230412191131_0124_m_000000_4122\\part-00000-d7bd89b9-48b7-4721-b40e-94b27ac9e4d8-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/04/12 19:11:31 WARN TaskSetManager: Lost task 0.0 in stage 124.0 (TID 4122, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmmpp\\fiiredataService_parquet\\fiiles\\_temporary\\0\\_temporary\\attempt_20230412191131_0124_m_000000_4122\\part-00000-d7bd89b9-48b7-4721-b40e-94b27ac9e4d8-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/04/12 19:11:31 ERROR TaskSetManager: Task 0 in stage 124.0 failed 1 times; aborting job\n",
      "23/04/12 19:11:31 ERROR FileFormatWriter: Aborting job 50832ae1-3018-4070-b3fe-96cda543fcf5.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 124.0 failed 1 times, most recent failure: Lost task 0.0 in stage 124.0 (TID 4122, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmmpp\\fiiredataService_parquet\\fiiles\\_temporary\\0\\_temporary\\attempt_20230412191131_0124_m_000000_4122\\part-00000-d7bd89b9-48b7-4721-b40e-94b27ac9e4d8-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:170)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat ammonite.$sess.cmd54$Helper.<init>(cmd54.sc:1)\n",
      "\tat ammonite.$sess.cmd54$.<init>(cmd54.sc:7)\n",
      "\tat ammonite.$sess.cmd54$.<clinit>(cmd54.sc)\n",
      "\tat ammonite.$sess.cmd54.$main(cmd54.sc)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$evalMain$1(Evaluator.scala:108)\n",
      "\tat ammonite.util.Util$.withContextClassloader(Util.scala:24)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:90)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$2(Evaluator.scala:127)\n",
      "\tat ammonite.util.Catching.map(Res.scala:117)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$1(Evaluator.scala:121)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:120)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$4(Interpreter.scala:295)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$2(Interpreter.scala:281)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.evaluateLine(Interpreter.scala:280)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$6(Interpreter.scala:268)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$4(Interpreter.scala:251)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$2(Interpreter.scala:244)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.processLine(Interpreter.scala:243)\n",
      "\tat almond.Execute.$anonfun$ammResult$11(Execute.scala:238)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$2(CaptureImpl.scala:53)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withErr(Console.scala:196)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$1(CaptureImpl.scala:45)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withOut(Console.scala:167)\n",
      "\tat almond.internals.CaptureImpl.apply(CaptureImpl.scala:45)\n",
      "\tat almond.Execute.capturingOutput(Execute.scala:166)\n",
      "\tat almond.Execute.$anonfun$ammResult$10(Execute.scala:225)\n",
      "\tat almond.Execute.$anonfun$withClientStdin$1(Execute.scala:146)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withIn(Console.scala:230)\n",
      "\tat almond.Execute.withClientStdin(Execute.scala:142)\n",
      "\tat almond.Execute.$anonfun$ammResult$9(Execute.scala:225)\n",
      "\tat almond.Execute.withInputManager(Execute.scala:134)\n",
      "\tat almond.Execute.$anonfun$ammResult$8(Execute.scala:224)\n",
      "\tat ammonite.repl.Signaller.apply(Signaller.scala:28)\n",
      "\tat almond.Execute.interruptible(Execute.scala:183)\n",
      "\tat almond.Execute.$anonfun$ammResult$7(Execute.scala:223)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat almond.Execute.$anonfun$ammResult$1(Execute.scala:214)\n",
      "\tat almond.Execute.withOutputHandler(Execute.scala:157)\n",
      "\tat almond.Execute.ammResult(Execute.scala:214)\n",
      "\tat almond.Execute.apply(Execute.scala:311)\n",
      "\tat almond.ScalaInterpreter.execute(ScalaInterpreter.scala:127)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter.$anonfun$execute$2(InterpreterToIOInterpreter.scala:69)\n",
      "\tat cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:366)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:387)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:330)\n",
      "\tat cats.effect.internals.IOShift$Tick.run(IOShift.scala:36)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmmpp\\fiiredataService_parquet\\fiiles\\_temporary\\0\\_temporary\\attempt_20230412191131_0124_m_000000_4122\\part-00000-d7bd89b9-48b7-4721-b40e-94b27ac9e4d8-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\r\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n",
      "\t... 3 more\r\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted.\u001b[39m\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m198\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m170\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\r\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\r\n  ammonite.$sess.cmd54$Helper.<init>(\u001b[32mcmd54.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd54$.<init>(\u001b[32mcmd54.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd54$.<clinit>(\u001b[32mcmd54.sc\u001b[39m:\u001b[32m-1\u001b[39m)\r\n\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 124.0 failed 1 times, most recent failure: Lost task 0.0 in stage 124.0 (TID 4122, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmmpp\\fiiredataService_parquet\\fiiles\\_temporary\\0\\_temporary\\attempt_20230412191131_0124_m_000000_4122\\part-00000-d7bd89b9-48b7-4721-b40e-94b27ac9e4d8-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\u001b[39m\r\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1891\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1879\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1878\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\r\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1878\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2112\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2061\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2050\u001b[39m)\r\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m738\u001b[39m)\r\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m167\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m170\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\r\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\r\n  ammonite.$sess.cmd54$Helper.<init>(\u001b[32mcmd54.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd54$.<init>(\u001b[32mcmd54.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd54$.<clinit>(\u001b[32mcmd54.sc\u001b[39m:\u001b[32m-1\u001b[39m)\r\n\u001b[31mjava.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmmpp\\fiiredataService_parquet\\fiiles\\_temporary\\0\\_temporary\\attempt_20230412191131_0124_m_000000_4122\\part-00000-d7bd89b9-48b7-4721-b40e-94b27ac9e4d8-c000.snappy.parquet\u001b[39m\r\n  org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(\u001b[32mShell.java\u001b[39m:\u001b[32m762\u001b[39m)\r\n  org.apache.hadoop.util.Shell.execCommand(\u001b[32mShell.java\u001b[39m:\u001b[32m859\u001b[39m)\r\n  org.apache.hadoop.util.Shell.execCommand(\u001b[32mShell.java\u001b[39m:\u001b[32m842\u001b[39m)\r\n  org.apache.hadoop.fs.RawLocalFileSystem.setPermission(\u001b[32mRawLocalFileSystem.java\u001b[39m:\u001b[32m661\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem$1.apply(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m501\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m482\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.setPermission(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m498\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.create(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m467\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.create(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m433\u001b[39m)\r\n  org.apache.hadoop.fs.FileSystem.create(\u001b[32mFileSystem.java\u001b[39m:\u001b[32m908\u001b[39m)\r\n  org.apache.hadoop.fs.FileSystem.create(\u001b[32mFileSystem.java\u001b[39m:\u001b[32m889\u001b[39m)\r\n  org.apache.parquet.hadoop.util.HadoopOutputFile.create(\u001b[32mHadoopOutputFile.java\u001b[39m:\u001b[32m74\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetFileWriter.<init>(\u001b[32mParquetFileWriter.java\u001b[39m:\u001b[32m248\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(\u001b[32mParquetOutputFormat.java\u001b[39m:\u001b[32m390\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(\u001b[32mParquetOutputFormat.java\u001b[39m:\u001b[32m349\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(\u001b[32mParquetOutputWriter.scala\u001b[39m:\u001b[32m37\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(\u001b[32mParquetFileFormat.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(\u001b[32mFileFormatDataWriter.scala\u001b[39m:\u001b[32m123\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(\u001b[32mFileFormatDataWriter.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m236\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m177\u001b[39m)\r\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m90\u001b[39m)\r\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\r\n  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m411\u001b[39m)\r\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\r\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\r\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\r\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\r\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\").save(\"/tmmpp/fiiredataService_parquet/fiiles/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18. Rechargez  les données stockées en format Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: Unable to infer schema for Parquet. It must be specified manually.;\u001b[39m\r\n  org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$13(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m185\u001b[39m)\r\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m185\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m373\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.loadV1Source(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m223\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.load(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m211\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.parquet(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m645\u001b[39m)\r\n  org.apache.spark.sql.DataFrameReader.parquet(\u001b[32mDataFrameReader.scala\u001b[39m:\u001b[32m629\u001b[39m)\r\n  ammonite.$sess.cmd55$Helper.<init>(\u001b[32mcmd55.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd55$.<init>(\u001b[32mcmd55.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd55$.<clinit>(\u001b[32mcmd55.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val newdataDF = sparkSession.read.parquet(\"/tmmpp/fiiredataService_parquet/fiiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd56.sc:1: not found: value newdataDF\n",
      "val res56 = newdataDF.printSchema\n",
      "            ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "newdataDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
